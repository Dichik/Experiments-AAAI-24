{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current location:  /Users/tarasbohdan/Desktop/Experiments-AAAI-24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cur_folder_name = os.getcwd().split('/')[-1]\n",
    "if cur_folder_name != \"fairness-variance\":\n",
    "    os.chdir(\"../..\")\n",
    "\n",
    "print('Current location: ', os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:33.207390Z",
     "start_time": "2023-10-28T10:26:32.319409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:34.852415Z",
     "start_time": "2023-10-28T10:26:32.331932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Config:\n",
    "  file_name = './data/student_performance/student-mat.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:34.905534Z",
     "start_time": "2023-10-28T10:26:34.694337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   age  Medu  Fedu  traveltime  studytime  failures  famrel  freetime  goout  \\\n0   18     4     4           2          2         0       4         3      4   \n1   17     1     1           1          2         0       5         3      3   \n2   15     1     1           1          2         3       4         3      2   \n3   15     4     2           1          3         0       3         2      2   \n4   16     3     3           1          2         0       4         3      2   \n\n   Dalc  Walc  health  absences     Mjob      Fjob  reason guardian sex  \n0     1     1       3         6  at_home   teacher  course   mother   F  \n1     1     1       3         4  at_home     other  course   father   F  \n2     2     3       3        10  at_home     other   other   mother   F  \n3     1     1       5         2   health  services    home   mother   F  \n4     1     2       5         4    other     other    home   father   F  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>Medu</th>\n      <th>Fedu</th>\n      <th>traveltime</th>\n      <th>studytime</th>\n      <th>failures</th>\n      <th>famrel</th>\n      <th>freetime</th>\n      <th>goout</th>\n      <th>Dalc</th>\n      <th>Walc</th>\n      <th>health</th>\n      <th>absences</th>\n      <th>Mjob</th>\n      <th>Fjob</th>\n      <th>reason</th>\n      <th>guardian</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>at_home</td>\n      <td>teacher</td>\n      <td>course</td>\n      <td>mother</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>at_home</td>\n      <td>other</td>\n      <td>course</td>\n      <td>father</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>10</td>\n      <td>at_home</td>\n      <td>other</td>\n      <td>other</td>\n      <td>mother</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>health</td>\n      <td>services</td>\n      <td>home</td>\n      <td>mother</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>other</td>\n      <td>other</td>\n      <td>home</td>\n      <td>father</td>\n      <td>F</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebooks.mult_repair_levels_student_performance.StudentPerformanceDataset import StudentPerformanceDataset\n",
    "\n",
    "data_loader = StudentPerformanceDataset(Config.file_name)\n",
    "data_loader.X_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:34.924544Z",
     "start_time": "2023-10-28T10:26:34.703129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "init_data_loader = copy.deepcopy(data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:34.924890Z",
     "start_time": "2023-10-28T10:26:34.791031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/l243p36n10s5v1x_vrt2wzs80000gn/T/ipykernel_24022/2079174263.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_loader.X_data['sex_binary'] = data_loader.X_data['sex'].apply(lambda x: 1 if x == 'M' else 0)\n",
      "/Users/tarasbohdan/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from virny.preprocessing import preprocess_dataset\n",
    "from source.preprocessing import get_simple_preprocessor\n",
    "\n",
    "data_loader.categorical_columns = [col for col in data_loader.categorical_columns if\n",
    "                                   col not in ('sex')]\n",
    "data_loader.X_data['sex_binary'] = data_loader.X_data['sex'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "data_loader.full_df = data_loader.full_df.drop(['sex'], axis=1)\n",
    "data_loader.X_data = data_loader.X_data.drop(['sex'], axis=1)\n",
    "\n",
    "# Preprocess the dataset using the defined preprocessor\n",
    "column_transformer = get_simple_preprocessor(data_loader)\n",
    "base_flow_dataset = preprocess_dataset(data_loader, column_transformer, 0.3, 42)\n",
    "base_flow_dataset.init_features_df = init_data_loader.full_df.drop(init_data_loader.target, axis=1,\n",
    "                                                                   errors='ignore')\n",
    "base_flow_dataset.X_train_val['sex_binary'] = data_loader.X_data.loc[\n",
    "    base_flow_dataset.X_train_val.index, 'sex_binary']\n",
    "base_flow_dataset.X_test['sex_binary'] = data_loader.X_data.loc[base_flow_dataset.X_test.index, 'sex_binary']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.718915Z",
     "start_time": "2023-10-28T10:26:34.796131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_df = data_loader.X_data\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = train_test_split(full_df, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.719712Z",
     "start_time": "2023-10-28T10:26:35.692911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sensitive_attribute = 'sex_binary'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.720205Z",
     "start_time": "2023-10-28T10:26:35.693981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_df = base_flow_dataset.X_train_val\n",
    "train_df[base_flow_dataset.target] = base_flow_dataset.y_train_val\n",
    "\n",
    "test_df = base_flow_dataset.X_test\n",
    "test_df[base_flow_dataset.target] = base_flow_dataset.y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.724203Z",
     "start_time": "2023-10-28T10:26:35.701869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_binary_dataset = BinaryLabelDataset(df=train_df,\n",
    "                                              label_names=[data_loader.target],\n",
    "                                              protected_attribute_names=[sensitive_attribute],\n",
    "                                              favorable_label=1,\n",
    "                                              unfavorable_label=0)\n",
    "test_binary_dataset = BinaryLabelDataset(df=test_df,\n",
    "                                         label_names=[data_loader.target],\n",
    "                                         protected_attribute_names=[sensitive_attribute],\n",
    "                                         favorable_label=1,\n",
    "                                         unfavorable_label=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.743727Z",
     "start_time": "2023-10-28T10:26:35.707268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original training dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.034286\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original test dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.154042\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "privileged_groups = [{'sex_binary': 1}]\n",
    "unprivileged_groups = [{'sex_binary': 0}]\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(train_binary_dataset,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(test_binary_dataset,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:35.763332Z",
     "start_time": "2023-10-28T10:26:35.720528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 1.4647248934252552, L_x: 1.2930002362289983,  L_y: 0.6375024072866353,  L_z: 0.016285849675526598\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          160     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.46472D+00    |proj g|=  5.50848D-01\n",
      "step: 250, loss: 1.451611624751507, L_x: 1.2929699849987861,  L_y: 0.6260357924639941,  L_z: 0.0162529226487505\n",
      "\n",
      "At iterate    1    f=  1.45161D+00    |proj g|=  5.69975D-01\n",
      "\n",
      "At iterate    2    f=  8.78809D-01    |proj g|=  5.29264D-01\n",
      "step: 500, loss: 5.640273739314181, L_x: 1.1967113097398432,  L_y: 0.615568455776723,  L_z: 0.1002547634088012\n",
      "step: 750, loss: 1.6614912860447353, L_x: 1.2979485766014707,  L_y: 0.6185427532961756,  L_z: 0.020599380939650896\n",
      "\n",
      "At iterate    3    f=  6.70450D-01    |proj g|=  5.58346D-01\n",
      "step: 1000, loss: 0.747391069731161, L_x: 1.2996741110165353,  L_y: 0.6214006871270966,  L_z: 0.0022598728298779793\n",
      "step: 1250, loss: 0.6711567664369729, L_x: 1.2994349858302365,  L_y: 0.621457604720904,  L_z: 0.0007340962371553295\n",
      "\n",
      "At iterate    4    f=  6.61561D-01    |proj g|=  5.56167D-01\n",
      "step: 1500, loss: 0.7393161549018171, L_x: 1.2987904196629123,  L_y: 0.6224762849787328,  L_z: 0.0020770393145291056\n",
      "step: 1750, loss: 0.647330854040483, L_x: 1.299282577510772,  L_y: 0.6216638606045838,  L_z: 0.0002534833532158276\n",
      "\n",
      "At iterate    5    f=  6.47331D-01    |proj g|=  5.56272D-01\n",
      "step: 2000, loss: 0.6431174683350385, L_x: 1.2992235973750277,  L_y: 0.6217867583262686,  L_z: 0.00016676948070039056\n",
      "\n",
      "At iterate    6    f=  6.43118D-01    |proj g|=  6.95319D-01\n",
      "step: 2250, loss: 0.6652266315434037, L_x: 1.2990128105336558,  L_y: 0.6222485330458206,  L_z: 0.0005997594078449298\n",
      "step: 2500, loss: 0.6433340030216798, L_x: 1.299219179115331,  L_y: 0.6217968426640696,  L_z: 0.00017089937132913624\n",
      "\n",
      "At iterate    7    f=  6.43101D-01    |proj g|=  5.25879D-01\n",
      "step: 2750, loss: 0.6402287959939528, L_x: 1.2992330296896355,  L_y: 0.6217460879529376,  L_z: 0.00010980755488237403\n",
      "\n",
      "At iterate    8    f=  6.40229D-01    |proj g|=  4.76320D-01\n",
      "step: 3000, loss: 0.6400964344991513, L_x: 1.2992222459995202,  L_y: 0.6216479442402353,  L_z: 0.00010912535597841777\n",
      "\n",
      "At iterate    9    f=  6.40097D-01    |proj g|=  5.59063D-01\n",
      "\n",
      "At iterate   10    f=  6.37609D-01    |proj g|=  5.55717D-01\n",
      "step: 3250, loss: 0.6460395629450456, L_x: 1.2992219551095558,  L_y: 0.6216576146074867,  L_z: 0.00022779457572926764\n",
      "step: 3500, loss: 0.636443017100907, L_x: 1.2992305559014607,  L_y: 0.6216340949069284,  L_z: 3.633233269927927e-05\n",
      "step: 3750, loss: 0.6360893361393727, L_x: 1.299230736040621,  L_y: 0.6216332378817124,  L_z: 2.9275817945079164e-05\n",
      "step: 4000, loss: 0.6361301008495215, L_x: 1.2992308297259973,  L_y: 0.6216329609935644,  L_z: 3.0096631173942567e-05\n",
      "\n",
      "At iterate   11    f=  6.36090D-01    |proj g|=  5.63755D-01\n",
      "step: 4250, loss: 0.635789134774378, L_x: 1.299225248680989,  L_y: 0.6216376159963766,  L_z: 2.318532582383237e-05\n",
      "\n",
      "At iterate   12    f=  6.35787D-01    |proj g|=  5.36063D-01\n",
      "step: 4500, loss: 0.6375364051782639, L_x: 1.2992120862671315,  L_y: 0.6216529900901867,  L_z: 5.782588450811699e-05\n",
      "\n",
      "At iterate   13    f=  6.35616D-01    |proj g|=  5.56601D-01\n",
      "step: 4750, loss: 0.6361696798295663, L_x: 1.29921409145307,  L_y: 0.6216153218657715,  L_z: 3.1244340985281394e-05\n",
      "step: 5000, loss: 0.6355622320441318, L_x: 1.2992220974691748,  L_y: 0.6216378752439359,  L_z: 1.8642716510081093e-05\n",
      "\n",
      "At iterate   14    f=  6.35562D-01    |proj g|=  5.64547D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  160     14     32     17     0     2   5.645D-01   6.356D-01\n",
      "  F =  0.63556190538964907     \n",
      "\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT        \n"
     ]
    }
   ],
   "source": [
    "TR = LFR(unprivileged_groups=unprivileged_groups,\n",
    "         privileged_groups=privileged_groups,\n",
    "         Az=50.0,\n",
    "         verbose=1\n",
    "        )\n",
    "TR = TR.fit(train_binary_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.044874Z",
     "start_time": "2023-10-28T10:26:35.734177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "dataset_transf_train = TR.transform(train_binary_dataset)\n",
    "dataset_transf_test = TR.transform(test_binary_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.050930Z",
     "start_time": "2023-10-28T10:26:37.042551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Transformed training dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Transformed test dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())\n",
    "metric_transf_test = BinaryLabelDatasetMetric(dataset_transf_test,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_test.mean_difference())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.064477Z",
     "start_time": "2023-10-28T10:26:37.049695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_df, _ = train_binary_dataset.convert_to_dataframe()\n",
    "test_df, _ = test_binary_dataset.convert_to_dataframe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.161005Z",
     "start_time": "2023-10-28T10:26:37.062923Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "     cat__Mjob_at_home  cat__Mjob_health  cat__Mjob_other  cat__Mjob_services  \\\n17                 0.0               0.0              1.0                 0.0   \n165                0.0               0.0              0.0                 1.0   \n24                 0.0               0.0              0.0                 1.0   \n126                0.0               0.0              1.0                 0.0   \n346                0.0               0.0              0.0                 0.0   \n176                0.0               0.0              0.0                 1.0   \n275                0.0               0.0              0.0                 1.0   \n246                0.0               0.0              1.0                 0.0   \n239                0.0               0.0              1.0                 0.0   \n153                0.0               0.0              0.0                 1.0   \n\n     cat__Mjob_teacher  cat__Fjob_at_home  cat__Fjob_health  cat__Fjob_other  \\\n17                 0.0                0.0               0.0              1.0   \n165                0.0                0.0               0.0              0.0   \n24                 0.0                0.0               1.0              0.0   \n126                0.0                0.0               0.0              1.0   \n346                1.0                0.0               0.0              0.0   \n176                0.0                0.0               0.0              1.0   \n275                0.0                0.0               0.0              1.0   \n246                0.0                0.0               0.0              1.0   \n239                0.0                0.0               0.0              0.0   \n153                0.0                1.0               0.0              0.0   \n\n     cat__Fjob_services  cat__Fjob_teacher  ...  num__failures  num__famrel  \\\n17                  0.0                0.0  ...      -0.462007     1.215287   \n165                 1.0                0.0  ...       0.894524     0.050637   \n24                  0.0                0.0  ...      -0.462007     0.050637   \n126                 0.0                0.0  ...      -0.462007     1.215287   \n346                 1.0                0.0  ...      -0.462007     1.215287   \n176                 0.0                0.0  ...      -0.462007    -1.114013   \n275                 0.0                0.0  ...      -0.462007     0.050637   \n246                 0.0                0.0  ...      -0.462007     1.215287   \n239                 1.0                0.0  ...       0.894524     1.215287   \n153                 0.0                0.0  ...       3.607587     0.050637   \n\n     num__freetime  num__goout  num__Dalc  num__Walc  num__health  \\\n17       -0.249043   -1.016419  -0.557247  -1.013096     0.322877   \n165       1.714836   -1.016419  -0.557247  -1.013096    -1.080495   \n24       -0.249043   -1.016419  -0.557247  -1.013096     1.024563   \n126      -0.249043   -1.016419  -0.557247  -1.013096    -1.782181   \n346      -0.249043   -1.016419  -0.557247  -0.247029     0.322877   \n176       0.732897    0.759098  -0.557247   1.285105     1.024563   \n275       0.732897    0.759098   0.565382   0.519038     1.024563   \n246      -1.230982   -1.016419  -0.557247  -1.013096    -1.080495   \n239       1.714836    0.759098   1.688010   2.051172    -1.080495   \n153       1.714836    0.759098  -0.557247  -1.013096     0.322877   \n\n     num__absences  sex_binary   G3  \n17       -0.221442         0.0  1.0  \n165       1.159755         1.0  1.0  \n24       -0.451641         0.0  0.0  \n126      -0.681841         0.0  1.0  \n346       0.354057         1.0  1.0  \n176      -0.451641         0.0  1.0  \n275       0.008758         0.0  1.0  \n246      -0.221442         1.0  1.0  \n239      -0.681841         1.0  0.0  \n153      -0.681841         1.0  0.0  \n\n[10 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__Mjob_at_home</th>\n      <th>cat__Mjob_health</th>\n      <th>cat__Mjob_other</th>\n      <th>cat__Mjob_services</th>\n      <th>cat__Mjob_teacher</th>\n      <th>cat__Fjob_at_home</th>\n      <th>cat__Fjob_health</th>\n      <th>cat__Fjob_other</th>\n      <th>cat__Fjob_services</th>\n      <th>cat__Fjob_teacher</th>\n      <th>...</th>\n      <th>num__failures</th>\n      <th>num__famrel</th>\n      <th>num__freetime</th>\n      <th>num__goout</th>\n      <th>num__Dalc</th>\n      <th>num__Walc</th>\n      <th>num__health</th>\n      <th>num__absences</th>\n      <th>sex_binary</th>\n      <th>G3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>1.215287</td>\n      <td>-0.249043</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>0.322877</td>\n      <td>-0.221442</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.894524</td>\n      <td>0.050637</td>\n      <td>1.714836</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>-1.080495</td>\n      <td>1.159755</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>0.050637</td>\n      <td>-0.249043</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>1.024563</td>\n      <td>-0.451641</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>1.215287</td>\n      <td>-0.249043</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>-1.782181</td>\n      <td>-0.681841</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>1.215287</td>\n      <td>-0.249043</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-0.247029</td>\n      <td>0.322877</td>\n      <td>0.354057</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>-1.114013</td>\n      <td>0.732897</td>\n      <td>0.759098</td>\n      <td>-0.557247</td>\n      <td>1.285105</td>\n      <td>1.024563</td>\n      <td>-0.451641</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>275</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>0.050637</td>\n      <td>0.732897</td>\n      <td>0.759098</td>\n      <td>0.565382</td>\n      <td>0.519038</td>\n      <td>1.024563</td>\n      <td>0.008758</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.462007</td>\n      <td>1.215287</td>\n      <td>-1.230982</td>\n      <td>-1.016419</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>-1.080495</td>\n      <td>-0.221442</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.894524</td>\n      <td>1.215287</td>\n      <td>1.714836</td>\n      <td>0.759098</td>\n      <td>1.688010</td>\n      <td>2.051172</td>\n      <td>-1.080495</td>\n      <td>-0.681841</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>3.607587</td>\n      <td>0.050637</td>\n      <td>1.714836</td>\n      <td>0.759098</td>\n      <td>-0.557247</td>\n      <td>-1.013096</td>\n      <td>0.322877</td>\n      <td>-0.681841</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.183833Z",
     "start_time": "2023-10-28T10:26:37.084261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.185428Z",
     "start_time": "2023-10-28T10:26:37.088151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_X = train_df.drop(['G3'], axis=1)\n",
    "train_y = train_df['G3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.185801Z",
     "start_time": "2023-10-28T10:26:37.164588Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "test_X = test_df.drop(['G3'], axis=1)\n",
    "test_y = test_df['G3']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.186568Z",
     "start_time": "2023-10-28T10:26:37.169452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "rfc = rfc.fit(train_X, train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.395642Z",
     "start_time": "2023-10-28T10:26:37.174974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(test_X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.396407Z",
     "start_time": "2023-10-28T10:26:37.316267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.28      0.42        46\n",
      "         1.0       0.68      0.96      0.80        73\n",
      "\n",
      "    accuracy                           0.70       119\n",
      "   macro avg       0.75      0.62      0.61       119\n",
      "weighted avg       0.73      0.70      0.65       119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_y, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.397237Z",
     "start_time": "2023-10-28T10:26:37.317138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "train_df, _ = dataset_transf_train.convert_to_dataframe()\n",
    "test_df, _ = dataset_transf_test.convert_to_dataframe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.397740Z",
     "start_time": "2023-10-28T10:26:37.317457Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "     cat__Mjob_at_home  cat__Mjob_health  cat__Mjob_other  cat__Mjob_services  \\\n17            0.282101          0.248208         0.484390            0.532686   \n165           0.249704          0.239742         0.447924            0.605743   \n24            0.286787          0.277285         0.510560            0.555359   \n126           0.283425          0.263831         0.503686            0.543200   \n346           0.318287          0.267461         0.513688            0.529047   \n176           0.282252          0.255658         0.481118            0.585136   \n275           0.286671          0.260240         0.487202            0.584699   \n246           0.267992          0.263812         0.508562            0.521976   \n239           0.308950          0.258256         0.491667            0.596990   \n153           0.262591          0.243234         0.459737            0.594076   \n\n     cat__Mjob_teacher  cat__Fjob_at_home  cat__Fjob_health  cat__Fjob_other  \\\n17            0.400076           0.438113          0.156397         0.726901   \n165           0.371744           0.501155          0.164897         0.692198   \n24            0.376523           0.423796          0.193318         0.630097   \n126           0.399975           0.430144          0.173585         0.688904   \n346           0.397335           0.413002          0.181395         0.686483   \n176           0.384880           0.468259          0.179183         0.680416   \n275           0.383513           0.462902          0.184416         0.669277   \n246           0.409976           0.416502          0.163040         0.702821   \n239           0.397620           0.473029          0.188614         0.689607   \n153           0.381572           0.488665          0.166380         0.699859   \n\n     cat__Fjob_services  cat__Fjob_teacher  ...  num__failures  num__famrel  \\\n17             0.607373           0.506497  ...       0.390084     0.743162   \n165            0.632031           0.516646  ...       0.430889     0.773674   \n24             0.549982           0.525848  ...       0.334830     0.744751   \n126            0.569008           0.506249  ...       0.359209     0.745229   \n346            0.561874           0.508954  ...       0.329536     0.736252   \n176            0.588223           0.508834  ...       0.376986     0.762485   \n275            0.578193           0.510370  ...       0.365513     0.761147   \n246            0.569001           0.503836  ...       0.368557     0.738356   \n239            0.568675           0.492481  ...       0.351665     0.767680   \n153            0.618974           0.509857  ...       0.414230     0.768845   \n\n     num__freetime  num__goout  num__Dalc  num__Walc  num__health  \\\n17        0.335698    0.414230   0.522159   0.552276     0.514006   \n165       0.377180    0.423218   0.477969   0.591852     0.483389   \n24        0.268252    0.396082   0.552517   0.595887     0.549265   \n126       0.306384    0.436113   0.559446   0.567496     0.504330   \n346       0.291603    0.420168   0.588532   0.568906     0.517000   \n176       0.340344    0.447044   0.550275   0.589141     0.473185   \n275       0.329208    0.446071   0.560322   0.593267     0.477210   \n246       0.298512    0.431070   0.543138   0.550714     0.520457   \n239       0.352486    0.505402   0.619021   0.591391     0.409478   \n153       0.369878    0.439144   0.507523   0.584657     0.471818   \n\n     num__absences  sex_binary   G3  \n17        0.608915         0.0  1.0  \n165       0.612473         1.0  1.0  \n24        0.588588         0.0  1.0  \n126       0.589389         0.0  1.0  \n346       0.577047         1.0  1.0  \n176       0.582843         0.0  1.0  \n275       0.578044         0.0  1.0  \n246       0.603434         1.0  1.0  \n239       0.547248         1.0  1.0  \n153       0.601322         1.0  1.0  \n\n[10 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__Mjob_at_home</th>\n      <th>cat__Mjob_health</th>\n      <th>cat__Mjob_other</th>\n      <th>cat__Mjob_services</th>\n      <th>cat__Mjob_teacher</th>\n      <th>cat__Fjob_at_home</th>\n      <th>cat__Fjob_health</th>\n      <th>cat__Fjob_other</th>\n      <th>cat__Fjob_services</th>\n      <th>cat__Fjob_teacher</th>\n      <th>...</th>\n      <th>num__failures</th>\n      <th>num__famrel</th>\n      <th>num__freetime</th>\n      <th>num__goout</th>\n      <th>num__Dalc</th>\n      <th>num__Walc</th>\n      <th>num__health</th>\n      <th>num__absences</th>\n      <th>sex_binary</th>\n      <th>G3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>0.282101</td>\n      <td>0.248208</td>\n      <td>0.484390</td>\n      <td>0.532686</td>\n      <td>0.400076</td>\n      <td>0.438113</td>\n      <td>0.156397</td>\n      <td>0.726901</td>\n      <td>0.607373</td>\n      <td>0.506497</td>\n      <td>...</td>\n      <td>0.390084</td>\n      <td>0.743162</td>\n      <td>0.335698</td>\n      <td>0.414230</td>\n      <td>0.522159</td>\n      <td>0.552276</td>\n      <td>0.514006</td>\n      <td>0.608915</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>0.249704</td>\n      <td>0.239742</td>\n      <td>0.447924</td>\n      <td>0.605743</td>\n      <td>0.371744</td>\n      <td>0.501155</td>\n      <td>0.164897</td>\n      <td>0.692198</td>\n      <td>0.632031</td>\n      <td>0.516646</td>\n      <td>...</td>\n      <td>0.430889</td>\n      <td>0.773674</td>\n      <td>0.377180</td>\n      <td>0.423218</td>\n      <td>0.477969</td>\n      <td>0.591852</td>\n      <td>0.483389</td>\n      <td>0.612473</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.286787</td>\n      <td>0.277285</td>\n      <td>0.510560</td>\n      <td>0.555359</td>\n      <td>0.376523</td>\n      <td>0.423796</td>\n      <td>0.193318</td>\n      <td>0.630097</td>\n      <td>0.549982</td>\n      <td>0.525848</td>\n      <td>...</td>\n      <td>0.334830</td>\n      <td>0.744751</td>\n      <td>0.268252</td>\n      <td>0.396082</td>\n      <td>0.552517</td>\n      <td>0.595887</td>\n      <td>0.549265</td>\n      <td>0.588588</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>0.283425</td>\n      <td>0.263831</td>\n      <td>0.503686</td>\n      <td>0.543200</td>\n      <td>0.399975</td>\n      <td>0.430144</td>\n      <td>0.173585</td>\n      <td>0.688904</td>\n      <td>0.569008</td>\n      <td>0.506249</td>\n      <td>...</td>\n      <td>0.359209</td>\n      <td>0.745229</td>\n      <td>0.306384</td>\n      <td>0.436113</td>\n      <td>0.559446</td>\n      <td>0.567496</td>\n      <td>0.504330</td>\n      <td>0.589389</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>0.318287</td>\n      <td>0.267461</td>\n      <td>0.513688</td>\n      <td>0.529047</td>\n      <td>0.397335</td>\n      <td>0.413002</td>\n      <td>0.181395</td>\n      <td>0.686483</td>\n      <td>0.561874</td>\n      <td>0.508954</td>\n      <td>...</td>\n      <td>0.329536</td>\n      <td>0.736252</td>\n      <td>0.291603</td>\n      <td>0.420168</td>\n      <td>0.588532</td>\n      <td>0.568906</td>\n      <td>0.517000</td>\n      <td>0.577047</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>0.282252</td>\n      <td>0.255658</td>\n      <td>0.481118</td>\n      <td>0.585136</td>\n      <td>0.384880</td>\n      <td>0.468259</td>\n      <td>0.179183</td>\n      <td>0.680416</td>\n      <td>0.588223</td>\n      <td>0.508834</td>\n      <td>...</td>\n      <td>0.376986</td>\n      <td>0.762485</td>\n      <td>0.340344</td>\n      <td>0.447044</td>\n      <td>0.550275</td>\n      <td>0.589141</td>\n      <td>0.473185</td>\n      <td>0.582843</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>275</th>\n      <td>0.286671</td>\n      <td>0.260240</td>\n      <td>0.487202</td>\n      <td>0.584699</td>\n      <td>0.383513</td>\n      <td>0.462902</td>\n      <td>0.184416</td>\n      <td>0.669277</td>\n      <td>0.578193</td>\n      <td>0.510370</td>\n      <td>...</td>\n      <td>0.365513</td>\n      <td>0.761147</td>\n      <td>0.329208</td>\n      <td>0.446071</td>\n      <td>0.560322</td>\n      <td>0.593267</td>\n      <td>0.477210</td>\n      <td>0.578044</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>0.267992</td>\n      <td>0.263812</td>\n      <td>0.508562</td>\n      <td>0.521976</td>\n      <td>0.409976</td>\n      <td>0.416502</td>\n      <td>0.163040</td>\n      <td>0.702821</td>\n      <td>0.569001</td>\n      <td>0.503836</td>\n      <td>...</td>\n      <td>0.368557</td>\n      <td>0.738356</td>\n      <td>0.298512</td>\n      <td>0.431070</td>\n      <td>0.543138</td>\n      <td>0.550714</td>\n      <td>0.520457</td>\n      <td>0.603434</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>0.308950</td>\n      <td>0.258256</td>\n      <td>0.491667</td>\n      <td>0.596990</td>\n      <td>0.397620</td>\n      <td>0.473029</td>\n      <td>0.188614</td>\n      <td>0.689607</td>\n      <td>0.568675</td>\n      <td>0.492481</td>\n      <td>...</td>\n      <td>0.351665</td>\n      <td>0.767680</td>\n      <td>0.352486</td>\n      <td>0.505402</td>\n      <td>0.619021</td>\n      <td>0.591391</td>\n      <td>0.409478</td>\n      <td>0.547248</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>0.262591</td>\n      <td>0.243234</td>\n      <td>0.459737</td>\n      <td>0.594076</td>\n      <td>0.381572</td>\n      <td>0.488665</td>\n      <td>0.166380</td>\n      <td>0.699859</td>\n      <td>0.618974</td>\n      <td>0.509857</td>\n      <td>...</td>\n      <td>0.414230</td>\n      <td>0.768845</td>\n      <td>0.369878</td>\n      <td>0.439144</td>\n      <td>0.507523</td>\n      <td>0.584657</td>\n      <td>0.471818</td>\n      <td>0.601322</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.398566Z",
     "start_time": "2023-10-28T10:26:37.337631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "     cat__Mjob_at_home  cat__Mjob_health  cat__Mjob_other  cat__Mjob_services  \\\n78            0.255103          0.240229         0.458210            0.576314   \n371           0.284422          0.254031         0.481565            0.573242   \n248           0.290882          0.271316         0.511648            0.528820   \n55            0.277791          0.265136         0.504119            0.567840   \n390           0.310056          0.258612         0.484527            0.595020   \n223           0.346796          0.271048         0.525740            0.532521   \n42            0.271172          0.270933         0.505507            0.548645   \n234           0.268484          0.259258         0.494635            0.540415   \n316           0.286992          0.254375         0.477040            0.605730   \n116           0.275361          0.255923         0.495979            0.542954   \n\n     cat__Mjob_teacher  cat__Fjob_at_home  cat__Fjob_health  cat__Fjob_other  \\\n78            0.384179           0.479387          0.156957         0.712790   \n371           0.386724           0.461068          0.174813         0.689389   \n248           0.388552           0.410683          0.180058         0.664009   \n55            0.406397           0.447894          0.178923         0.686083   \n390           0.378187           0.469811          0.191686         0.668112   \n223           0.407354           0.410276          0.190369         0.692770   \n42            0.387270           0.427226          0.180661         0.655630   \n234           0.396373           0.433663          0.165938         0.693776   \n316           0.384104           0.484259          0.184561         0.678132   \n116           0.408934           0.439139          0.163169         0.715557   \n\n     cat__Fjob_services  cat__Fjob_teacher  ...  num__failures  num__famrel  \\\n78             0.628966           0.511989  ...       0.425016     0.762400   \n371            0.593217           0.509517  ...       0.378610     0.757735   \n248            0.561522           0.520080  ...       0.342124     0.735734   \n55             0.556985           0.495552  ...       0.358886     0.756976   \n390            0.580605           0.510143  ...       0.353438     0.763813   \n223            0.543471           0.496932  ...       0.301752     0.737158   \n42             0.560849           0.518502  ...       0.355884     0.745265   \n234            0.584009           0.510923  ...       0.378652     0.744997   \n316            0.586582           0.504072  ...       0.375855     0.771240   \n116            0.582370           0.498013  ...       0.378262     0.748113   \n\n     num__freetime  num__goout  num__Dalc  num__Walc  num__health  \\\n78        0.366995    0.419080   0.483931   0.572562     0.495721   \n371       0.338354    0.435808   0.543480   0.581992     0.485394   \n248       0.274145    0.391217   0.547390   0.573660     0.555698   \n55        0.321671    0.485969   0.587986   0.574810     0.452647   \n390       0.338153    0.450312   0.582537   0.602267     0.461525   \n223       0.294916    0.460792   0.648938   0.569928     0.472964   \n42        0.283985    0.409384   0.538456   0.580782     0.537696   \n234       0.311279    0.416687   0.525410   0.563776     0.523475   \n316       0.355603    0.473112   0.569097   0.598257     0.439563   \n116       0.329192    0.454304   0.553245   0.556024     0.482431   \n\n     num__absences  sex_binary   G3  \n78        0.616989         1.0  1.0  \n371       0.588732         1.0  1.0  \n248       0.597483         1.0  1.0  \n55        0.571043         0.0  1.0  \n390       0.563557         1.0  1.0  \n223       0.547068         1.0  1.0  \n42        0.598076         1.0  1.0  \n234       0.606460         1.0  1.0  \n316       0.568892         0.0  1.0  \n116       0.593116         1.0  1.0  \n\n[10 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__Mjob_at_home</th>\n      <th>cat__Mjob_health</th>\n      <th>cat__Mjob_other</th>\n      <th>cat__Mjob_services</th>\n      <th>cat__Mjob_teacher</th>\n      <th>cat__Fjob_at_home</th>\n      <th>cat__Fjob_health</th>\n      <th>cat__Fjob_other</th>\n      <th>cat__Fjob_services</th>\n      <th>cat__Fjob_teacher</th>\n      <th>...</th>\n      <th>num__failures</th>\n      <th>num__famrel</th>\n      <th>num__freetime</th>\n      <th>num__goout</th>\n      <th>num__Dalc</th>\n      <th>num__Walc</th>\n      <th>num__health</th>\n      <th>num__absences</th>\n      <th>sex_binary</th>\n      <th>G3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>78</th>\n      <td>0.255103</td>\n      <td>0.240229</td>\n      <td>0.458210</td>\n      <td>0.576314</td>\n      <td>0.384179</td>\n      <td>0.479387</td>\n      <td>0.156957</td>\n      <td>0.712790</td>\n      <td>0.628966</td>\n      <td>0.511989</td>\n      <td>...</td>\n      <td>0.425016</td>\n      <td>0.762400</td>\n      <td>0.366995</td>\n      <td>0.419080</td>\n      <td>0.483931</td>\n      <td>0.572562</td>\n      <td>0.495721</td>\n      <td>0.616989</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>371</th>\n      <td>0.284422</td>\n      <td>0.254031</td>\n      <td>0.481565</td>\n      <td>0.573242</td>\n      <td>0.386724</td>\n      <td>0.461068</td>\n      <td>0.174813</td>\n      <td>0.689389</td>\n      <td>0.593217</td>\n      <td>0.509517</td>\n      <td>...</td>\n      <td>0.378610</td>\n      <td>0.757735</td>\n      <td>0.338354</td>\n      <td>0.435808</td>\n      <td>0.543480</td>\n      <td>0.581992</td>\n      <td>0.485394</td>\n      <td>0.588732</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>0.290882</td>\n      <td>0.271316</td>\n      <td>0.511648</td>\n      <td>0.528820</td>\n      <td>0.388552</td>\n      <td>0.410683</td>\n      <td>0.180058</td>\n      <td>0.664009</td>\n      <td>0.561522</td>\n      <td>0.520080</td>\n      <td>...</td>\n      <td>0.342124</td>\n      <td>0.735734</td>\n      <td>0.274145</td>\n      <td>0.391217</td>\n      <td>0.547390</td>\n      <td>0.573660</td>\n      <td>0.555698</td>\n      <td>0.597483</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>0.277791</td>\n      <td>0.265136</td>\n      <td>0.504119</td>\n      <td>0.567840</td>\n      <td>0.406397</td>\n      <td>0.447894</td>\n      <td>0.178923</td>\n      <td>0.686083</td>\n      <td>0.556985</td>\n      <td>0.495552</td>\n      <td>...</td>\n      <td>0.358886</td>\n      <td>0.756976</td>\n      <td>0.321671</td>\n      <td>0.485969</td>\n      <td>0.587986</td>\n      <td>0.574810</td>\n      <td>0.452647</td>\n      <td>0.571043</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>0.310056</td>\n      <td>0.258612</td>\n      <td>0.484527</td>\n      <td>0.595020</td>\n      <td>0.378187</td>\n      <td>0.469811</td>\n      <td>0.191686</td>\n      <td>0.668112</td>\n      <td>0.580605</td>\n      <td>0.510143</td>\n      <td>...</td>\n      <td>0.353438</td>\n      <td>0.763813</td>\n      <td>0.338153</td>\n      <td>0.450312</td>\n      <td>0.582537</td>\n      <td>0.602267</td>\n      <td>0.461525</td>\n      <td>0.563557</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>0.346796</td>\n      <td>0.271048</td>\n      <td>0.525740</td>\n      <td>0.532521</td>\n      <td>0.407354</td>\n      <td>0.410276</td>\n      <td>0.190369</td>\n      <td>0.692770</td>\n      <td>0.543471</td>\n      <td>0.496932</td>\n      <td>...</td>\n      <td>0.301752</td>\n      <td>0.737158</td>\n      <td>0.294916</td>\n      <td>0.460792</td>\n      <td>0.648938</td>\n      <td>0.569928</td>\n      <td>0.472964</td>\n      <td>0.547068</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.271172</td>\n      <td>0.270933</td>\n      <td>0.505507</td>\n      <td>0.548645</td>\n      <td>0.387270</td>\n      <td>0.427226</td>\n      <td>0.180661</td>\n      <td>0.655630</td>\n      <td>0.560849</td>\n      <td>0.518502</td>\n      <td>...</td>\n      <td>0.355884</td>\n      <td>0.745265</td>\n      <td>0.283985</td>\n      <td>0.409384</td>\n      <td>0.538456</td>\n      <td>0.580782</td>\n      <td>0.537696</td>\n      <td>0.598076</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>0.268484</td>\n      <td>0.259258</td>\n      <td>0.494635</td>\n      <td>0.540415</td>\n      <td>0.396373</td>\n      <td>0.433663</td>\n      <td>0.165938</td>\n      <td>0.693776</td>\n      <td>0.584009</td>\n      <td>0.510923</td>\n      <td>...</td>\n      <td>0.378652</td>\n      <td>0.744997</td>\n      <td>0.311279</td>\n      <td>0.416687</td>\n      <td>0.525410</td>\n      <td>0.563776</td>\n      <td>0.523475</td>\n      <td>0.606460</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>0.286992</td>\n      <td>0.254375</td>\n      <td>0.477040</td>\n      <td>0.605730</td>\n      <td>0.384104</td>\n      <td>0.484259</td>\n      <td>0.184561</td>\n      <td>0.678132</td>\n      <td>0.586582</td>\n      <td>0.504072</td>\n      <td>...</td>\n      <td>0.375855</td>\n      <td>0.771240</td>\n      <td>0.355603</td>\n      <td>0.473112</td>\n      <td>0.569097</td>\n      <td>0.598257</td>\n      <td>0.439563</td>\n      <td>0.568892</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.275361</td>\n      <td>0.255923</td>\n      <td>0.495979</td>\n      <td>0.542954</td>\n      <td>0.408934</td>\n      <td>0.439139</td>\n      <td>0.163169</td>\n      <td>0.715557</td>\n      <td>0.582370</td>\n      <td>0.498013</td>\n      <td>...</td>\n      <td>0.378262</td>\n      <td>0.748113</td>\n      <td>0.329192</td>\n      <td>0.454304</td>\n      <td>0.553245</td>\n      <td>0.556024</td>\n      <td>0.482431</td>\n      <td>0.593116</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.399024Z",
     "start_time": "2023-10-28T10:26:37.353849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       119\n",
      "\n",
      "    accuracy                           1.00       119\n",
      "   macro avg       1.00      1.00      1.00       119\n",
      "weighted avg       1.00      1.00      1.00       119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, _ = dataset_transf_train.convert_to_dataframe()\n",
    "test_df, _ = dataset_transf_test.convert_to_dataframe()\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "train_X = train_df.drop(['G3'], axis=1)\n",
    "train_y = train_df['G3']\n",
    "\n",
    "test_X = test_df.drop(['G3'], axis=1)\n",
    "test_y = test_df['G3']\n",
    "\n",
    "rfc = rfc.fit(train_X, train_y)\n",
    "\n",
    "y_pred = rfc.predict(test_X)\n",
    "\n",
    "report = classification_report(test_y, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:28:27.965660Z",
     "start_time": "2023-10-28T10:28:27.846014Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tune parameters for LFR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.2096919259442772, L_x: 1.356051463361366,  L_y: 0.61390326509763,  L_z: 0.012696453098377613\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          160     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.09692D-01    |proj g|=  1.08365D-02\n",
      "step: 250, loss: 0.20865769056356984, L_x: 1.3543497905570838,  L_y: 0.6138755838033865,  L_z: 0.011835153127522775\n",
      "\n",
      "At iterate    1    f=  2.08658D-01    |proj g|=  1.08922D-02\n",
      "  ys=-3.344E-06  -gs= 1.033E-03 BFGS update SKIPPED\n",
      "step: 500, loss: 0.20410230529342385, L_x: 1.345706705183642,  L_y: 0.6137634988811697,  L_z: 0.008155284886942666\n",
      "\n",
      "At iterate    2    f=  2.04102D-01    |proj g|=  1.20085D-02\n",
      "step: 750, loss: 0.22185574618190845, L_x: 1.2500383209930321,  L_y: 0.6140621450228668,  L_z: 0.03544569958031858\n",
      "\n",
      "At iterate    3    f=  1.97524D-01    |proj g|=  9.98540D-03\n",
      "step: 1000, loss: 0.20109266661164496, L_x: 1.296063749109992,  L_y: 0.613102707770244,  L_z: 0.010176020923621337\n",
      "step: 1250, loss: 0.1974845481275879, L_x: 1.318540750025114,  L_y: 0.6134706302719272,  L_z: 0.004283410097883744\n",
      "\n",
      "At iterate    4    f=  1.97485D-01    |proj g|=  1.14709D-02\n",
      "\n",
      "At iterate    5    f=  1.96136D-01    |proj g|=  1.26263D-02\n",
      "step: 1500, loss: 0.19445871154367658, L_x: 1.3105940998237418,  L_y: 0.6131769595906474,  L_z: 0.0020816056022376684\n",
      "step: 1750, loss: 0.19375226400566695, L_x: 1.2857748197903875,  L_y: 0.6126022492955431,  L_z: 0.003914557097073867\n",
      "\n",
      "At iterate    6    f=  1.93752D-01    |proj g|=  1.08016D-02\n",
      "\n",
      "At iterate    7    f=  1.91117D-01    |proj g|=  9.55586D-03\n",
      "step: 2000, loss: 0.19020115685240782, L_x: 1.1679396775945747,  L_y: 0.6117729866326163,  L_z: 0.012229890429688712\n",
      "\n",
      "At iterate    8    f=  1.90201D-01    |proj g|=  1.40251D-02\n",
      "step: 2250, loss: 0.18501383493163334, L_x: 1.136712972955325,  L_y: 0.6106819324367554,  L_z: 0.010274344392425316\n",
      "\n",
      "At iterate    9    f=  1.72487D-01    |proj g|=  1.52395D-02\n",
      "step: 2500, loss: 0.16583881913953266, L_x: 0.9776392202531199,  L_y: 0.5991591687560043,  L_z: 0.008158980238620255\n",
      "\n",
      "At iterate   10    f=  1.65839D-01    |proj g|=  1.31958D-02\n",
      "\n",
      "At iterate   11    f=  1.63079D-01    |proj g|=  1.56082D-02\n",
      "step: 2750, loss: 0.16134753222707757, L_x: 0.9553443555434753,  L_y: 0.59151431315249,  L_z: 0.006661665357481017\n",
      "\n",
      "At iterate   12    f=  1.61348D-01    |proj g|=  1.51836D-02\n",
      "step: 3000, loss: 0.17898063800829245, L_x: 0.9804719075394037,  L_y: 0.6171262810995977,  L_z: 0.01922081914439231\n",
      "\n",
      "At iterate   13    f=  1.58139D-01    |proj g|=  1.12506D-02\n",
      "step: 3250, loss: 0.15800176878055414, L_x: 0.9451983928042216,  L_y: 0.5849891592145599,  L_z: 0.004983013578675965\n",
      "\n",
      "At iterate   14    f=  1.58002D-01    |proj g|=  1.24221D-02\n",
      "step: 3500, loss: 0.15531587052920964, L_x: 0.9458159505286068,  L_y: 0.5852205035362114,  L_z: 0.0022122251227278344\n",
      "\n",
      "At iterate   15    f=  1.55276D-01    |proj g|=  1.25938D-02\n",
      "step: 3750, loss: 0.15437686456024355, L_x: 0.9432233835283947,  L_y: 0.5862270123221471,  L_z: 0.001431824975189333\n",
      "\n",
      "At iterate   16    f=  1.54377D-01    |proj g|=  1.30110D-02\n",
      "step: 4000, loss: 0.15372712054964874, L_x: 0.9438097071977949,  L_y: 0.5850029595572893,  L_z: 0.0008458538741403065\n",
      "\n",
      "At iterate   17    f=  1.53727D-01    |proj g|=  1.09871D-02\n",
      "step: 4250, loss: 0.1532642350623723, L_x: 0.9443057257821631,  L_y: 0.5850013289671477,  L_z: 0.000333529587441217\n",
      "\n",
      "At iterate   18    f=  1.53264D-01    |proj g|=  1.14198D-02\n",
      "step: 4500, loss: 0.1546366370382583, L_x: 0.9448207450580419,  L_y: 0.5849111501237862,  L_z: 0.0016634475200754562\n",
      "\n",
      "At iterate   19    f=  1.53199D-01    |proj g|=  1.55263D-02\n",
      "step: 4750, loss: 0.15342566739920882, L_x: 0.9427360655846101,  L_y: 0.5847767323996473,  L_z: 0.0006743876007830885\n",
      "\n",
      "At iterate   20    f=  1.53044D-01    |proj g|=  1.23875D-02\n",
      "step: 5000, loss: 0.15338929412331181, L_x: 0.9424366993559994,  L_y: 0.5845934625856586,  L_z: 0.0006862779291459842\n",
      "step: 5250, loss: 0.15299224402841433, L_x: 0.9435677328147298,  L_y: 0.5848416431783905,  L_z: 0.00015130642910226032\n",
      "\n",
      "At iterate   21    f=  1.52992D-01    |proj g|=  1.28390D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  160     21     33     22     1     1   1.284D-02   1.530D-01\n",
      "  F =  0.15299216204118582     \n",
      "\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT        \n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.83      0.91         6\n",
      "         1.0       0.99      1.00      1.00       113\n",
      "\n",
      "    accuracy                           0.99       119\n",
      "   macro avg       1.00      0.92      0.95       119\n",
      "weighted avg       0.99      0.99      0.99       119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TR = LFR(unprivileged_groups=unprivileged_groups,\n",
    "         privileged_groups=privileged_groups,\n",
    "         k=5, Ax=0.1, Ay=0.1, Az=1,\n",
    "         verbose=1\n",
    "        )\n",
    "TR = TR.fit(train_binary_dataset, maxiter=5000, maxfun=5000)\n",
    "\n",
    "dataset_transf_train = TR.transform(train_binary_dataset)\n",
    "dataset_transf_test = TR.transform(test_binary_dataset)\n",
    "\n",
    "train_df, _ = dataset_transf_train.convert_to_dataframe()\n",
    "test_df, _ = dataset_transf_test.convert_to_dataframe()\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "train_X = train_df.drop(['G3'], axis=1)\n",
    "train_y = train_df['G3']\n",
    "\n",
    "test_X = test_df.drop(['G3'], axis=1)\n",
    "test_y = test_df['G3']\n",
    "\n",
    "rfc = rfc.fit(train_X, train_y)\n",
    "\n",
    "y_pred = rfc.predict(test_X)\n",
    "\n",
    "report = classification_report(test_y, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:28:38.869643Z",
     "start_time": "2023-10-28T10:28:37.482997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import LFR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# k - Number of prototypes\n",
    "# Input recontruction quality - Ax\n",
    "# Fairness constraint - Az\n",
    "# Output prediction error - Ay\n",
    "def find_optimal():\n",
    "\n",
    "    k_values = [1, 2, 3, 4, 5]\n",
    "    Az_values = [1.0, 5.0, 25.0, 50.0]\n",
    "    Ay_values = [1.0, 2.0, 3.0, 4.0, 10.0]\n",
    "    Ax_values = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_params = None\n",
    "\n",
    "    for k in k_values:\n",
    "        for Az in Az_values:\n",
    "            for Ay in Ay_values:\n",
    "                for Ax in Ax_values:\n",
    "                    lfr = LFR(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups,\n",
    "                              k=k, Az=Az, Ay=Ay, Ax=Ax)\n",
    "\n",
    "                    lfr = lfr.fit(train_binary_dataset)\n",
    "\n",
    "                    dataset_transf_train = lfr.transform(train_binary_dataset)\n",
    "\n",
    "                    train_df, _ = dataset_transf_train.convert_to_dataframe()\n",
    "\n",
    "                    train_X = train_df.drop(['G3'], axis=1)\n",
    "                    train_y = train_df['G3']\n",
    "\n",
    "                    rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "                    rfc.fit(train_X, train_y)\n",
    "\n",
    "                    dataset_transf_test = lfr.transform(test_binary_dataset)\n",
    "                    test_df, _ = dataset_transf_test.convert_to_dataframe()\n",
    "                    test_X = test_df.drop(['G3'], axis=1)\n",
    "                    test_y = test_df['G3']\n",
    "                    y_pred = rfc.predict(test_X)\n",
    "\n",
    "                    accuracy = accuracy_score(test_y, y_pred)\n",
    "\n",
    "                    if accuracy > best_accuracy and accuracy != 1.0:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_params = (k, Az, Ay, Ax)\n",
    "\n",
    "                    params = (k, Az, Ay, Ax)\n",
    "                    print(\"Parameters (k, Az, Ay, Ax):\", params)\n",
    "                    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "    return best_accuracy, best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:26:37.862539Z",
     "start_time": "2023-10-28T10:26:37.647394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 3.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 10.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 1.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 3.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 10.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 5.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 3.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 10.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 25.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 3.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 10.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (1, 50.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 3.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 4.0, 0.1)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 4.0, 10.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 10.0, 0.01)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 10.0, 0.1)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 10.0, 1.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 1.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 2.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 3.0, 1.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 3.0, 10.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 10.0, 0.1)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 5.0, 10.0, 10.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 2.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 2.0, 0.1)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 2.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 2.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 3.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 3.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 3.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 3.0, 10.0)\n",
      "Accuracy: 0.9915966386554622\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 4.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 4.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 4.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 4.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 10.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 10.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 10.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 25.0, 10.0, 10.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 50.0, 1.0, 0.01)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 50.0, 1.0, 0.1)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 50.0, 1.0, 1.0)\n",
      "Accuracy: 1.0\n",
      "Parameters (k, Az, Ay, Ax): (2, 50.0, 1.0, 10.0)\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m best_accuracy, best_params \u001B[38;5;241m=\u001B[39m \u001B[43mfind_optimal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Parameters (k, Az, Ay, Ax):\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Accuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_accuracy)\n",
      "Cell \u001B[0;32mIn[28], line 26\u001B[0m, in \u001B[0;36mfind_optimal\u001B[0;34m()\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m Ax \u001B[38;5;129;01min\u001B[39;00m Ax_values:\n\u001B[1;32m     23\u001B[0m     lfr \u001B[38;5;241m=\u001B[39m LFR(unprivileged_groups\u001B[38;5;241m=\u001B[39munprivileged_groups, privileged_groups\u001B[38;5;241m=\u001B[39mprivileged_groups,\n\u001B[1;32m     24\u001B[0m               k\u001B[38;5;241m=\u001B[39mk, Az\u001B[38;5;241m=\u001B[39mAz, Ay\u001B[38;5;241m=\u001B[39mAy, Ax\u001B[38;5;241m=\u001B[39mAx)\n\u001B[0;32m---> 26\u001B[0m     lfr \u001B[38;5;241m=\u001B[39m \u001B[43mlfr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_binary_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     dataset_transf_train \u001B[38;5;241m=\u001B[39m lfr\u001B[38;5;241m.\u001B[39mtransform(train_binary_dataset)\n\u001B[1;32m     30\u001B[0m     train_df, _ \u001B[38;5;241m=\u001B[39m dataset_transf_train\u001B[38;5;241m.\u001B[39mconvert_to_dataframe()\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/aif360/algorithms/transformer.py:27\u001B[0m, in \u001B[0;36maddmetadata.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 27\u001B[0m     new_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_dataset, Dataset):\n\u001B[1;32m     29\u001B[0m         new_dataset\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m new_dataset\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/aif360/algorithms/preprocessing/lfr.py:98\u001B[0m, in \u001B[0;36mLFR.fit\u001B[0;34m(self, dataset, maxiter, maxfun)\u001B[0m\n\u001B[1;32m     95\u001B[0m bnd \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)]\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk \u001B[38;5;241m+\u001B[39m [(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)]\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_dim\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk\n\u001B[1;32m     96\u001B[0m lfr_helpers\u001B[38;5;241m.\u001B[39mLFR_optim_objective\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearned_model \u001B[38;5;241m=\u001B[39m \u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmin_l_bfgs_b\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlfr_helpers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLFR_optim_objective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters_initialization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfeatures_unprivileged\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures_privileged\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mlabels_unprivileged\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_privileged\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_interval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapprox_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxfun\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaxfun\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mmaxiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaxiter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearned_model[:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk]\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprototypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearned_model[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk:]\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_dim))\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:199\u001B[0m, in \u001B[0;36mfmin_l_bfgs_b\u001B[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001B[0m\n\u001B[1;32m    187\u001B[0m callback \u001B[38;5;241m=\u001B[39m _wrap_callback(callback)\n\u001B[1;32m    188\u001B[0m opts \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisp\u001B[39m\u001B[38;5;124m'\u001B[39m: disp,\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124miprint\u001B[39m\u001B[38;5;124m'\u001B[39m: iprint,\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaxcor\u001B[39m\u001B[38;5;124m'\u001B[39m: m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcallback\u001B[39m\u001B[38;5;124m'\u001B[39m: callback,\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaxls\u001B[39m\u001B[38;5;124m'\u001B[39m: maxls}\n\u001B[0;32m--> 199\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43m_minimize_lbfgsb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m                       \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m d \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgrad\u001B[39m\u001B[38;5;124m'\u001B[39m: res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjac\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    202\u001B[0m      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask\u001B[39m\u001B[38;5;124m'\u001B[39m: res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    203\u001B[0m      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfuncalls\u001B[39m\u001B[38;5;124m'\u001B[39m: res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnfev\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    204\u001B[0m      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnit\u001B[39m\u001B[38;5;124m'\u001B[39m: res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnit\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    205\u001B[0m      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwarnflag\u001B[39m\u001B[38;5;124m'\u001B[39m: res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m'\u001B[39m]}\n\u001B[1;32m    206\u001B[0m f \u001B[38;5;241m=\u001B[39m res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfun\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001B[0m, in \u001B[0;36m_minimize_lbfgsb\u001B[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001B[0m\n\u001B[1;32m    359\u001B[0m task_str \u001B[38;5;241m=\u001B[39m task\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task_str\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFG\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;66;03m# The minimization routine wants f and g at the current x.\u001B[39;00m\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001B[39;00m\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;66;03m# until the completion of the current minimization iteration.\u001B[39;00m\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;66;03m# Overwrite f and g:\u001B[39;00m\n\u001B[0;32m--> 365\u001B[0m     f, g \u001B[38;5;241m=\u001B[39m \u001B[43mfunc_and_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m task_str\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNEW_X\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;66;03m# new iteration\u001B[39;00m\n\u001B[1;32m    368\u001B[0m     n_iterations \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:286\u001B[0m, in \u001B[0;36mScalarFunction.fun_and_grad\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_x_impl(x)\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_fun()\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mg\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:256\u001B[0m, in \u001B[0;36mScalarFunction._update_grad\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_grad\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mg_updated:\n\u001B[0;32m--> 256\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_grad_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mg_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:173\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.update_grad\u001B[0;34m()\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_fun()\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mngev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 173\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mg \u001B[38;5;241m=\u001B[39m \u001B[43mapprox_derivative\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun_wrapped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m                           \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfinite_diff_options\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_numdiff.py:505\u001B[0m, in \u001B[0;36mapprox_derivative\u001B[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001B[0m\n\u001B[1;32m    502\u001B[0m     use_one_sided \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparsity \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 505\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_dense_difference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun_wrapped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m                             \u001B[49m\u001B[43muse_one_sided\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    508\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m issparse(sparsity) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sparsity) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_numdiff.py:576\u001B[0m, in \u001B[0;36m_dense_difference\u001B[0;34m(fun, x0, f0, h, use_one_sided, method)\u001B[0m\n\u001B[1;32m    574\u001B[0m     x \u001B[38;5;241m=\u001B[39m x0 \u001B[38;5;241m+\u001B[39m h_vecs[i]\n\u001B[1;32m    575\u001B[0m     dx \u001B[38;5;241m=\u001B[39m x[i] \u001B[38;5;241m-\u001B[39m x0[i]  \u001B[38;5;66;03m# Recompute dx as exactly representable number.\u001B[39;00m\n\u001B[0;32m--> 576\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m f0\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m3-point\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m use_one_sided[i]:\n\u001B[1;32m    578\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m x0 \u001B[38;5;241m+\u001B[39m h_vecs[i]\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_numdiff.py:456\u001B[0m, in \u001B[0;36mapprox_derivative.<locals>.fun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfun_wrapped\u001B[39m(x):\n\u001B[0;32m--> 456\u001B[0m     f \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39matleast_1d(\u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m f\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    458\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`fun` return value has \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    459\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmore than 1 dimension.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnfev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# Send a copy because the user may overwrite it.\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Overwriting results in undefined behaviour because\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m fx \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;66;03m# Make sure the function returns a true scalar\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(fx):\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:17\u001B[0m, in \u001B[0;36mLFR_optim_objective\u001B[0;34m(parameters, x_unprivileged, x_privileged, y_unprivileged, y_privileged, k, A_x, A_y, A_z, print_interval, verbose)\u001B[0m\n\u001B[1;32m     14\u001B[0m w \u001B[38;5;241m=\u001B[39m parameters[:k]\n\u001B[1;32m     15\u001B[0m prototypes \u001B[38;5;241m=\u001B[39m parameters[k:]\u001B[38;5;241m.\u001B[39mreshape((k, features_dim))\n\u001B[0;32m---> 17\u001B[0m M_unprivileged, x_hat_unprivileged, y_hat_unprivileged \u001B[38;5;241m=\u001B[39m \u001B[43mget_xhat_y_hat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprototypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_unprivileged\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m M_privileged, x_hat_privileged, y_hat_privileged \u001B[38;5;241m=\u001B[39m get_xhat_y_hat(prototypes, w, x_privileged)\n\u001B[1;32m     21\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([y_hat_unprivileged, y_hat_privileged], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Experiments-AAAI-24/virny_venv/lib/python3.9/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:41\u001B[0m, in \u001B[0;36mget_xhat_y_hat\u001B[0;34m(prototypes, w, x)\u001B[0m\n\u001B[1;32m     39\u001B[0m M \u001B[38;5;241m=\u001B[39m softmax(\u001B[38;5;241m-\u001B[39mcdist(x, prototypes), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     40\u001B[0m x_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmatmul(M, prototypes)\n\u001B[0;32m---> 41\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m M, x_hat, y_hat\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mclip\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy, best_params = find_optimal()\n",
    "print(\"Best Parameters (k, Az, Ay, Ax):\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T10:28:13.275170Z",
     "start_time": "2023-10-28T10:26:37.650810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-28T10:28:13.266963Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
